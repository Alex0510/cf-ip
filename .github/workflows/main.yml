name: Fetch IP Addresses

on:
  # 每3小时运行一次
  schedule:
    - cron: '0 */3 * * *'
  # 允许手动触发
  workflow_dispatch:

jobs:
  fetch-ips:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        # 获取完整历史记录，以便后续提交
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4

    - name: Run IP fetcher script
      run: |
        cat > fetch_ips.py << 'EOF'
        import requests
        from bs4 import BeautifulSoup
        import re
        import os

        # 目标URL列表
        urls = [
            'https://ip.164746.xyz', 
            'https://cf.090227.xyz', 
            'https://stock.hostmonit.com/CloudFlareYes',
            'https://www.wetest.vip/page/cloudflare/address_v4.html'
        ]

        # 正则表达式用于匹配IP地址
        ip_pattern = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'

        # 检查ip.txt文件是否存在,如果存在则删除它
        if os.path.exists('ip.txt'):
            os.remove('ip.txt')

        # 使用集合存储IP地址实现自动去重
        unique_ips = set()

        for url in urls:
            try:
                # 添加User-Agent头以避免被某些网站阻止
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                
                # 发送HTTP请求获取网页内容
                response = requests.get(url, headers=headers, timeout=10)
                
                # 确保请求成功
                if response.status_code == 200:
                    # 获取网页的文本内容
                    html_content = response.text
                    
                    # 使用正则表达式查找IP地址
                    ip_matches = re.findall(ip_pattern, html_content, re.IGNORECASE)
                    
                    # 将找到的IP添加到集合中（自动去重）
                    unique_ips.update(ip_matches)
                    print(f'从 {url} 找到 {len(ip_matches)} 个IP地址')
                else:
                    print(f'请求 {url} 失败，状态码: {response.status_code}')
            except requests.exceptions.RequestException as e:
                print(f'请求 {url} 失败: {e}')
                continue

        # 将去重后的IP地址按数字顺序排序后写入文件
        if unique_ips:
            # 按IP地址的数字顺序排序（非字符串顺序）
            sorted_ips = sorted(unique_ips, key=lambda ip: [int(part) for part in ip.split('.')])
            
            with open('ip.txt', 'w') as file:
                for ip in sorted_ips:
                    file.write(ip + '\n')
            print(f'已保存 {len(sorted_ips)} 个唯一IP地址到ip.txt文件。')
        else:
            print('未找到有效的IP地址。')
            # 创建一个空文件，避免后续步骤出错
            with open('ip.txt', 'w') as file:
                file.write('')
        EOF
        
        python fetch_ips.py

    - name: Commit and push if changed
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add ip.txt
        # 检查是否有更改
        if git diff --staged --quiet; then
          echo "没有更改，跳过提交"
        else
          git commit -m "自动更新IP地址列表 - $(date +'%Y-%m-%d %H:%M:%S')"
          git push
        fi
